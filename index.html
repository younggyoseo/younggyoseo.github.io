<!DOCTYPE HTML>
<html lang="en">

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FQNMP9HZQG"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-FQNMP9HZQG');
  </script>

  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Younggyo Seo</title>

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link href="css/bootstrap.min.css" rel="stylesheet" media="screen">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

  <meta name="author" content="Younggyo Seo">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŽ“</text></svg>">
  <style>
    .container {
      display: flex;
      flex-wrap: wrap;
      width: 100%;
      border: 0px;
      border-spacing: 0px;
      border-collapse: separate;
      margin-right: auto;
      margin-left: auto;
      align-items: center;
      /* Center-aligns items vertically in desktop mode */
    }

    .text-content {
      padding: 2.5%;
      width: 63%;
      vertical-align: middle;
      order: 1;
    }

    .image-content {
      padding: 2.5%;
      width: 37%;
      max-width: 37%;
      order: 2;
      display: flex;
      justify-content: center;
      /* Center-aligns image horizontally */
    }

    .image-content img {
      width: 100%;
      max-width: 100%;
    }

    @media screen and (max-width: 768px) {

      .text-content,
      .image-content {
        width: 100%;
        max-width: 100%;
        order: 1;
      }

      .image-content {
        order: 2;
        display: flex;
        justify-content: center;
        /* Center-aligns image horizontally in mobile mode */
      }

      .image-content img {
        width: auto;
        max-width: 100%;
        height: auto;
        max-height: 300px;
        /* Adjust this value as needed */
      }
    }
  </style>
</head>

<body class="bg_colour">
  <table <table border=0 class="bg_colour"
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <div class="container">
            <div class="text-content">
              <p style="text-align:left">
                <name>Younggyo Seo</name>
              </p>
              <p>Hi! I am a researcher at Amazon Frontier AI & Robotics (FAR) team, where I work with <a
                  href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>.</p>

              <p>
                My goal is to develop intelligent robots that achieve super-human performance that go
                beyond the level of human-generated behaviors.
                To that end, my research focuses on reinforcement learning that enables agents to discover new
                behaviors through online experiences.
                To further enable such agents to understand the world, my research also focuses on
                world models, video generation, and representation learning.
              </p>

              <p>Previously, I was a postdoctoral scholar at UC Berkeley working with <a
                  href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a> and also spent a year as
                a research scientist at Dyson Robot Learning Lab working with <a
                  href="https://stepjam.github.io/">Stephen James</a>, training robots with reinforcement learning.
                Before that, I received my PhD from KAIST advised by <a
                  href="https://alinlab.kaist.ac.kr/shin.html">Jinwoo Shin</a>.
                During my PhD, I was a visiting scholar at UC Berkeley working with <a
                  href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a> and <a
                  href="https://sites.google.com/view/kiminlee">Kimin Lee</a> and interned at
                Microsoft Research Asia.
              </p>
              <p>Feel free to send me an e-mail if you want to have a chat!<br><b>Contact</b>: mail AT younggyo.me</p>
              <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=tI1-YwIAAAAJ&hl=en">Google Scholar</a>&nbsp/&nbsp
                <a href="https://twitter.com/younggyoseo">Twitter</a>&nbsp/&nbsp
                <a href="https://github.com/younggyoseo">Github</a>&nbsp/&nbsp
                <a href="https://drive.google.com/file/d/1BGFSHIJdX-Ab8lKsHQS7QtDLodMZo2un/view?usp=sharing">CV (Jul.
                  2025)</a>
              </p>
            </div>
            <div class="image-content">
              <a href="images/younggyo.jpg"><img alt="profile photo" src="images/younggyo.jpg"
                  class="hoverZoomLink"></a>
            </div>
          </div>
          <hr class="soft">

          <button style="border:0px transparent; background-color: transparent;outline:none;" type="button"
            class="collapsible" data-toggle="collapse" data-target="#content-research" id="research">
            <heading>Publications</heading>
          </button>
          <div id="content-research" class="collapse in">
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr onmouseout="fast_td3_stop()" onmouseover="fast_td3_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/fast_td3.gif' width="160">
                    </div>
                    <script type="text/javascript">
                      function fast_td3_start() {
                        document.getElementById('fast_td3_image').style.opacity = "1";
                      }

                      function fast_td3_stop() {
                        document.getElementById('fast_td3_image').style.opacity = "0";
                      }
                      fast_td3_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://younggyo.me/fast_td3/">
                      <papertitle>FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control
                      </papertitle>
                    </a>
                    <br>
                    <strong>Younggyo Seo</strong>,
                    <a href="https://sferrazza.cc/">Carmelo Sferrazza</a>,
                    <a href="https://geng-haoran.github.io/">Haoran Geng</a>,
                    <a href="https://scholar.google.com/citations?user=GnEVRtQAAAAJ&hl=en">Michal Nauman</a>,
                    <a href="https://zhaohengyin.github.io/">Zhao-Heng Yin</a>,
                    <a href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>
                    <br>
                    <em>Technical Report</em>, 2025
                    <br>
                    <a href="https://arxiv.org/abs/2505.22642">Paper</a> / <a
                      href="https://younggyo.me/fast_td3/">Website</a>
                    / <a href="https://github.com/younggyoseo/FastTD3">Code</a>
                    <p>We introduce FastTD3, a simple, fast, and capable off-policy RL algorithm for humanoid control.
                    </p>
                  </td>
                </tr>
                <tr onmouseout="robot_r1_stop()" onmouseover="robot_r1_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/robot_r1.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function robot_r1_start() {
                        document.getElementById('robot_r1_image').style.opacity = "1";
                      }

                      function robot_r1_stop() {
                        document.getElementById('robot_r1_image').style.opacity = "0";
                      }
                      robot_r1_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2506.00070">
                      <papertitle>Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://github.com/kingdy2002">Dongyoung Kim</a>,
                    Sumin Park,
                    <a href="https://huiwon-jang.github.io/">Huiwon Jang</a>,
                    <a href="http://alinlab.kaist.ac.kr/shin.html">Jinwoo Shin</a>,
                    <a href="https://junsu-kim97.github.io/">Jaehyung Kim</a>,
                    <strong>Younggyo Seo</strong>,
                    <br>
                    <em>Neural Information Processing Systems (<u>NeurIPS</u>)</em>, 2025.
                    <br>
                    <a href="https://arxiv.org/abs/2506.00070">Paper</a>
                    <p> We introduce Robot-R1, a novel framework that leverages reinforcement learning to enhance embodied reasoning specifically for robot control.</p>
                  </td>
                </tr>
                <tr onmouseout="cqn_as_stop()" onmouseover="cqn_as_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/cqn_as.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function cqn_as_start() {
                        document.getElementById('cqn_as_image').style.opacity = "1";
                      }

                      function cqn_as_stop() {
                        document.getElementById('cqn_as_image').style.opacity = "0";
                      }
                      cqn_as_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://younggyo.me/cqn-as/">
                      <papertitle>Coarse-to-fine Q-Network with Action Sequence for Data-Efficient Robot Learning
                      </papertitle>
                    </a>
                    <br>
                    <strong>Younggyo Seo</strong>,
                    <a href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>
                    <br>
                    <em>Neural Information Processing Systems (<u>NeurIPS</u>)</em>, 2025.
                    <br>
                    <a href="https://arxiv.org/abs/2411.12155">Paper</a> / <a
                      href="https://younggyo.me/cqn-as/">Website</a>
                    / <a href="https://github.com/younggyoseo/CQN-AS">Code</a>
                    <p>We present Coarse-to-fine Q-Network with Action Sequence (CQN-<b>AS</b>), a value-based RL
                      algorithm that trains a critic network to output Q-values over <em>a sequence of actions</em>.</p>
                  </td>
                </tr>
                <tr onmouseout="coordtok_stop()" onmouseover="coordtok_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/coordtok.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function coordtok_start() {
                        document.getElementById('coordtok_image').style.opacity = "1";
                      }

                      function coordtok_stop() {
                        document.getElementById('coordtok_image').style.opacity = "0";
                      }
                      coordtok_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://huiwon-jang.github.io/coordtok/">
                      <papertitle>Efficient Long Video Tokenization via Coordinate-based Patch Reconstruction
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://huiwon-jang.github.io/">Huiwon Jang</a>,
                    <a href="https://sihyun.me/">Sihyun Yu</a>,
                    <a href="http://alinlab.kaist.ac.kr/shin.html">Jinwoo Shin</a>,
                    <a href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
                    <strong>Younggyo Seo</strong>
                    <br>
                    <em>Conference on Computer Vision and Pattern Recognition (<u>CVPR</u>)</em>, 2025.
                    <br>
                    <a href="https://arxiv.org/abs/2411.14762">Paper</a> / <a
                      href="https://huiwon-jang.github.io/coordtok/">Website</a>
                    / <a href="https://github.com/huiwon-jang/CoordTok">Code</a>
                    <p>We introduce CoordTok, a scalable video tokenizer that learns a mapping from coordinate-based
                      representations to the corresponding patches of input videos.
                    </p>
                  </td>
                </tr>
                <tr>
                  <td colspan="2" style="padding:0px">
                    <hr class="soft">
                  </td>
                </tr>

                <tr onmouseout="cqn_stop()" onmouseover="cqn_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/cqn.gif' width="160">
                    </div>
                    <script type="text/javascript">
                      function cqn_start() {
                        document.getElementById('cqn_image').style.opacity = "1";
                      }

                      function cqn_stop() {
                        document.getElementById('cqn_image').style.opacity = "0";
                      }
                      cqn_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://younggyo.me/cqn/">
                      <papertitle>Continuous Control with Coarse-to-fine Reinforcement Learning
                      </papertitle>
                    </a>
                    <br>
                    <strong>Younggyo Seo</strong>,
                    <a href="https://github.com/JafarAbdi">Jafar UruÃ§</a>,
                    <a href="https://stepjam.github.io/">Stephen James</a>
                    <br>
                    <em>Conference on Robot Learning (<u>CoRL</u>)</em>, 2024.
                    <br>
                    <a href="https://arxiv.org/abs/2407.07787">Paper</a> / <a
                      href="https://younggyo.me/cqn/">Website</a> / <a
                      href="https://github.com/younggyoseo/CQN">Code</a>
                    <p>We present Coarse-to-fine Reinforcement Learning (CRL), which trains RL agents to zoom-into
                      continuous action space in a coarse-to-fine manner. Within this framework, we present
                      Coarse-to-fine Q-Network (CQN), a
                      value-based RL algorithm for continuous control.</p>
                  </td>
                </tr>

                <tr onmouseout="bigym_stop()" onmouseover="bigym_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/bigym.gif' width="160">
                    </div>
                    <script type="text/javascript">
                      function bigym_start() {
                        document.getElementById('bigym_image').style.opacity = "1";
                      }

                      function bigym_stop() {
                        document.getElementById('bigym_image').style.opacity = "0";
                      }
                      bigym_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://chernyadev.github.io/bigym//">
                      <papertitle>BiGym: A Demo-Driven Mobile Bi-Manual Manipulation Benchmark
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://www.linkedin.com/in/nikita-chernyadev-8495417a/">Nikita Cherniadev*</a>,
                    <a href="https://www.linkedin.com/in/nicholas-backshall/">Nicholas Backshall*</a>,
                    <a href="https://yusufma03.github.io/">Xiao Ma*</a>,
                    <a href="https://www.linkedin.com/in/yunfan-lu-90170992/?originalSubdomain=sg">Yunfan Lu</a>,
                    <strong>Younggyo Seo</strong>,
                    <a href="https://stepjam.github.io/">Stephen James</a>
                    <br>
                    <em>Conference on Robot Learning (<u>CoRL</u>)</em>, 2024.
                    <br>
                    <a href="https://arxiv.org/abs/2407.07788">Paper</a> / <a
                      href="https://chernyadev.github.io/bigym/">Website</a> / <a
                      href="https://github.com/chernyadev/bigym">Code</a>
                    <p>We present BiGym, a new benchmark and learning environment for mobile bi-manual demo-driven
                      robotic manipulation. BiGym consists of 40 diverse tasks in home environment, and provides
                      human-collected
                      demonstrations.</p>
                  </td>
                </tr>

                <tr onmouseout="m3l_stop()" onmouseover="m3l_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/m3l.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function m3l_start() {
                        document.getElementById('m3l_image').style.opacity = "1";
                      }

                      function m3l_stop() {
                        document.getElementById('m3l_image').style.opacity = "0";
                      }
                      m3l_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://sferrazza.cc/m3l_site/">
                      <papertitle>The Power of the Senses: Generalizable Manipulation from Vision and Touch through
                        Masked Multimodal Learning
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://sferrazza.cc/">Carmelo Sferrazza</a>,
                    <strong>Younggyo Seo</strong>,
                    <a href="https://www.haoliu.site/">Hao Liu</a>,
                    <a href="https://youngwoon.github.io/">Youngwoon Lee</a>,
                    <a href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>
                    <br>
                    <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<u>IROS</u>)</em>, 2024.
                    <br>
                    <!-- <em>RSS Workshop on Interdisciplinary Exploration of Generalizable Manipulation Policy Learning:
                      Paradigms and Debates</em>, 2023
                    <br> -->
                    <a href="https://arxiv.org/abs/2311.00924">Paper</a> / <a
                      href="https://sferrazza.cc/m3l_site/">Website</a> /
                    <a href="https://github.com/carlosferrazza/M3L">Code</a>
                    <p>We propose Masked Multimodal Learning (M3L), which jointly learns a policy and visual-tactile
                      representations based on masked autoencoding.</p>
                  </td>
                </tr>

                <tr onmouseout="rsp_stop()" onmouseover="rsp_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/rsp.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function rsp_start() {
                        document.getElementById('rsp_image').style.opacity = "1";
                      }

                      function rsp_stop() {
                        document.getElementById('rsp_image').style.opacity = "0";
                      }
                      rsp_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2406.07398">
                      <papertitle> Visual Representation Learning with Stochastic Frame Prediction
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://huiwon-jang.github.io/">Huiwon Jang</a>,
                    <a href="https://github.com/kingdy2002">Dongyoung Kim</a>,
                    <a href="https://junsu-kim97.github.io/">Junsu Kim</a>,
                    <a href="http://alinlab.kaist.ac.kr/shin.html">Jinwoo Shin</a>,
                    <a href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
                    <strong>Younggyo Seo</strong>
                    <br>
                    <em>International Conference on Machine Learning (<u>ICML</u>)</em>, 2024.
                    <br>
                    <a href="https://arxiv.org/abs/2406.07398">Paper</a> / <a
                      href="https://sites.google.com/view/2024rsp">Website</a>
                    <p> We present RSP, a framework for visual representation learning from videos, that learns
                      representations that capture temporal information between frames by training a stochastic future
                      frame prediction model.</p>
                  </td>
                </tr>

                <tr onmouseout="rnd_stop()" onmouseover="rnd_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/rnd.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function rnd_start() {
                        document.getElementById('rnd_image').style.opacity = "1";
                      }

                      function rsp_stop() {
                        document.getElementById('rnd_image').style.opacity = "0";
                      }
                      rnd_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2405.18196">
                      <papertitle>Render and Diffuse: Aligning Image and Action Spaces for Diffusion-based Behaviour
                        Cloning
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://x.com/vitalisvos19/">Vitalis Vosylius</a>,
                    <strong>Younggyo Seo</strong>,
                    <a href="https://github.com/JafarAbdi">Jafar UruÃ§</a>,
                    <a href="https://stepjam.github.io/">Stephen James</a>,
                    <br>
                    <em>Robotics: Science and Systems (<u>RSS</u>)</em>, 2024.
                    <br>
                    <a href="https://arxiv.org/abs/2405.18196">Paper</a>
                    / <a href="https://vv19.github.io/render-and-diffuse/">Website</a>
                    <p> We introduce Render and Diffuse (R&D), a method that unifies low-level robot actions and RGB
                      observations within the image space using virtual renders of
                      the 3D model of the robot.</p>
                  </td>
                </tr>

                <tr>
                  <td colspan="2" style="padding:0px">
                    <hr class="soft">
                  </td>
                </tr>

                <tr onmouseout="vcse_stop()" onmouseover="vcse_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/vcse2.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function vcse_start() {
                        document.getElementById('vcse_image').style.opacity = "1";
                      }

                      function vcse_stop() {
                        document.getElementById('vcse_image').style.opacity = "0";
                      }
                      vcse_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2305.19476">
                      <papertitle>Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://github.com/kingdy2002">Dongyoung Kim</a>,
                    <a href="http://alinlab.kaist.ac.kr/shin.html">Jinwoo Shin</a>,
                    <a href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
                    <strong>Younggyo Seo</strong>
                    <br>
                    <em>Neural Information Processing Systems (<u>NeurIPS</u>)</em>, 2023.
                    <br>
                    <a href="https://arxiv.org/abs/2305.19476">Paper</a>
                    / <a href="https://sites.google.com/view/rl-vcse">Website</a>
                    / <a href="https://github.com/kingdy2002/VCSE">Code</a>
                    <p>We introduce a new exploration technique that maximizes value-conditional state entropy, which
                      takes into account the value estimates of states for computing the intrinsic bonus.</p>
                  </td>
                </tr>

                <tr onmouseout="arp_stop()" onmouseover="arp_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/mrdt.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function arp_start() {
                        document.getElementById('arp_image').style.opacity = "1";
                      }

                      function arp_stop() {
                        document.getElementById('arp_image').style.opacity = "0";
                      }
                      arp_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2309.10790">
                      <papertitle>Guide Your Agents with Adaptive Multimodal Rewards
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://changyeon.page/">Changyeon Kim</a>,
                    <strong>Younggyo Seo</strong>,
                    <a href="https://www.haoliu.site/">Hao Liu</a>,
                    <a href="https://leelisa.com/">Lisa Lee</a>,
                    <a href="https://web.eecs.umich.edu/~honglak/">Honglak Lee</a>,
                    <a href="http://alinlab.kaist.ac.kr/shin.html">Jinwoo Shin</a>,
                    <a href="https://sites.google.com/view/kiminlee">Kimin Lee</a>
                    <br>
                    <em>Neural Information Processing Systems (<u>NeurIPS</u>)</em>, 2023.
                    <br>
                    <em>ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems</em>, 2023.
                    <br>
                    <a href="https://arxiv.org/abs/2309.10790">Paper</a>
                    / <a href="https://sites.google.com/view/2023arp">Website</a> / <a
                      href="https://github.com/csmile-1006/ARP">Code</a>
                    <p>We present ARP (Adaptive Return-conditioned Policy), which utilizes an adaptive multimodal
                      reward signal for behavior learning.</p>
                  </td>
                </tr>

                <tr onmouseout="mvmwm_stop()" onmouseover="mvmwm_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/mvmwm.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function mvmwm_start() {
                        document.getElementById('mvmwm_image').style.opacity = "1";
                      }

                      function mvmwm_stop() {
                        document.getElementById('mvmwm_image').style.opacity = "0";
                      }
                      mvmwm_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2302.02408">
                      <papertitle>Multi-View Masked World Models for Visual Robotic Manipulation
                      </papertitle>
                    </a>
                    <br>
                    <strong>Younggyo Seo*</strong>,
                    <a href="https://junsu-kim97.github.io/">Junsu Kim*</a>,
                    <a href="https://stepjam.github.io/">Stephen James</a>,
                    <a href="https://sites.google.com/view/kiminlee">Kimin Lee</a>,
                    <a href="http://alinlab.kaist.ac.kr/shin.html">Jinwoo Shin</a>,
                    <a href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>
                    <br>
                    <em>International Conference on Machine Learning (<u>ICML</u>)</em>, 2023.
                    <br>
                    <em>RSS Workshop on Experiment-oriented Locomotion and Manipulation Research</em>, 2023 as
                    <b>Spotlight presentation</b>.
                    <br>
                    <a href="https://arxiv.org/abs/2302.02408">Paper</a>
                    / <a href="https://sites.google.com/view/mv-mwm">Website</a> / <a
                      href="https://github.com/younggyoseo/MV-MWM">Code</a>
                    <p>We introduce MV-MWM that learns multi-view representations via masked view reconstruction and
                      utilize them for visual robotic
                      manipulation.</p>
                  </td>
                </tr>

                <tr onmouseout="lamp_stop()" onmouseover="lamp_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/lamp.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function lamp_start() {
                        document.getElementById('lamp_image').style.opacity = "1";
                      }

                      function lamp_stop() {
                        document.getElementById('lamp_image').style.opacity = "0";
                      }
                      lamp_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2308.12270">
                      <papertitle>Language Reward Modulation for Pretraining Reinforcement Learning
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://ademiadeniji.github.io/">Ademi Adeniji</a>,
                    <a href="https://amberxie88.github.io/">Amber Xie</a>,
                    <a href="https://sferrazza.cc/">Carmelo Sferrazza</a>,
                    <strong>Younggyo Seo</strong>,
                    <a href="https://stepjam.github.io/">Stephen James</a>,
                    <a href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>
                    <br>
                    <em>Preprint</em>, 2023
                    <br>
                    <a href="https://arxiv.org/abs/2308.12270">Paper</a>
                    / <a href="https://github.com/ademiadeniji/lamp">Code</a>
                    <p>We introduce LAMP, a method for pretraining RL agents by using multimodal reward signal from
                      Video-Langauge models.
                    </p>
                  </td>
                </tr>

                <tr onmouseout="pig_stop()" onmouseover="pig_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/pig.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function pig_start() {
                        document.getElementById('pig_image').style.opacity = "1";
                      }

                      function pig_stop() {
                        document.getElementById('pig_image').style.opacity = "0";
                      }
                      pig_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://openreview.net/forum?id=6lUEy1J5R7p">
                      <papertitle>Imitating Graph-Based Planning with Goal-Conditioned Policies
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://junsu-kim97.github.io/">Junsu Kim</a>,
                    <strong>Younggyo Seo</strong>,
                    <a href="https://sungsooahn.notion.site/">Sungsoo Ahn</a>,
                    <a href="https://scholar.google.com/citations?user=y9n5wc4AAAAJ&hl=ko">Kyunghwan Son</a>,
                    <a href="http://alinlab.kaist.ac.kr/shin.html">Jinwoo Shin</a>
                    <br>
                    <em>International Conference on Learning Representations (<u>ICLR</u>)</em>, 2023.
                    <br>
                    <a href="https://openreview.net/forum?id=6lUEy1J5R7p">Paper</a>
                    / <a href="https://openreview.net/forum?id=6lUEy1J5R7p">Code</a>
                    <p>We introduce PIG, a simple yet effective self-imitation scheme which distills a
                      subgoal-conditioned policy into the target-goal-conditioned policy. </p>
                  </td>
                </tr>

                <tr>
                  <td colspan="2" style="padding:0px">
                    <hr class="soft">
                  </td>
                </tr>

                <tr onmouseout="mwm_stop()" onmouseover="mwm_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/mwm.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function mwm_start() {
                        document.getElementById('mwm_image').style.opacity = "1";
                      }

                      function mwm_stop() {
                        document.getElementById('mwm_image').style.opacity = "0";
                      }
                      mwm_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2206.14244">
                      <papertitle>Masked World Models for Visual Control
                      </papertitle>
                    </a>
                    <br>
                    <strong>Younggyo Seo</strong>,
                    <a href="https://danijar.com/">Danijar Hafner</a>,
                    <a href="https://www.haoliu.site/">Hao Liu</a>,
                    <a href="https://fangchenliu.github.io/">Fangchen Liu</a>,
                    <a href="https://stepjam.github.io/">Stephen James</a>,
                    <a href="https://sites.google.com/view/kiminlee">Kimin Lee</a>,
                    <a href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>
                    <br>
                    <em>Conference on Robot Learning (<u>CoRL</u>)</em>, 2022.
                    <br>
                    <a href="https://arxiv.org/abs/2206.14244">Paper</a>
                    / <a href="https://sites.google.com/view/mwm-rl">Website</a> / <a
                      href="https://github.com/younggyoseo/MWM">Code</a>
                    <p>We introduce MWM that learns a latent dynamics model on top of an autoencoder trained with
                      convolutional feature masking and reward prediction.</p>
                  </td>
                </tr>

                <tr onmouseout="dadt_stop()" onmouseover="dadt_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/dadt.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function dadt_start() {
                        document.getElementById('dadt_image').style.opacity = "1";
                      }

                      function dadt_stop() {
                        document.getElementById('dadt_image').style.opacity = "0";
                      }
                      surf_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://openreview.net/forum?id=ReNyLYfUdr">
                      <papertitle>Dynamics-Augmented Decision Transformer for Offline Dynamics Generalization
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://changyeon.page/">Changyeon Kim*</a>,
                    <a href="https://sites.google.com/view/junsu-kim">Junsu Kim*</a>,
                    <strong>Younggyo Seo</strong>,
                    <a href="https://sites.google.com/view/kiminlee">Kimin Lee</a>,
                    <a href="https://web.eecs.umich.edu/~honglak/">Honglak Lee</a>,
                    <a href="http://alinlab.kaist.ac.kr/shin.html">Jinwoo Shin</a>
                    <br>
                    <em>NeurIPS Workshop on Offline Reinforcement Learning</em>, 2022.
                    <br>
                    <a href="https://openreview.net/forum?id=ReNyLYfUdr">Paper</a>
                    <p>We introduce DADT, which improves dynamics generalization of decision transformer by introducing
                      a next-state prediction objective.</p>
                  </td>
                </tr>

                <tr onmouseout="apv_stop()" onmouseover="apv_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/apv.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function apv_start() {
                        document.getElementById('apv_image').style.opacity = "1";
                      }

                      function apv_stop() {
                        document.getElementById('apv_image').style.opacity = "0";
                      }
                      apv_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2203.13880">
                      <papertitle>Reinforcement Learning with Action-Free Pre-Training from Videos
                      </papertitle>
                    </a>
                    <br>
                    <strong>Younggyo Seo</strong>,
                    <a href="https://sites.google.com/view/kiminlee">Kimin Lee</a>,
                    <a href="https://stepjam.github.io/">Stephen James</a>,
                    <a href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>
                    <br>
                    <em>International Conference on Machine Learning (<u>ICML</u>)</em>, 2022.
                    <br>
                    <a href="https://arxiv.org/abs/2203.13880">Paper</a> / <a
                      href="https://sites.google.com/view/rl-apv">Website</a> / <a
                      href="https://github.com/younggyoseo/apv">Code</a>
                    <p>We introduce APV that can leverage diverse videos from different domains for pre-training to
                      improve sample-efficiency.</p>
                  </td>
                </tr>

                <tr onmouseout="harp_stop()" onmouseover="harp_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/harp.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function harp_start() {
                        document.getElementById('harp_image').style.opacity = "1";
                      }

                      function harp_stop() {
                        document.getElementById('harp_image').style.opacity = "0";
                      }
                      harp_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2209.07143">
                      <papertitle>HARP: Autoregressive Latent Video Prediction with High-Fidelity Image Generator
                      </papertitle>
                    </a>
                    <br>
                    <strong>Younggyo Seo</strong>,
                    <a href="https://sites.google.com/view/kiminlee">Kimin Lee</a>,
                    <a href="https://fangchenliu.github.io/">Fangchen Liu</a>,
                    <a href="https://stepjam.github.io/">Stephen James</a>,
                    <a href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>
                    <br>
                    <em>International Conference on Image Processing (<u>ICIP</u>)</em>, 2022.
                    <br>
                    <a href="https://arxiv.org/abs/2209.07143">Paper</a>
                    <p>We introduce a video prediction model that can generate 256x256 frames by training an
                      autorgressive transformer on top of VQ-GAN.</p>
                  </td>
                </tr>

                <tr onmouseout="surf_stop()" onmouseover="surf_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/surf.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function surf_start() {
                        document.getElementById('surf_image').style.opacity = "1";
                      }

                      function surf_stop() {
                        document.getElementById('surf_image').style.opacity = "0";
                      }
                      surf_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2209.07143">
                      <papertitle>SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient
                        Preference-based Reinforcement Learning
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://pjj4288.github.io/">Jongjin Park</a>,
                    <strong>Younggyo Seo</strong>,
                    <a href="http://alinlab.kaist.ac.kr/shin.html">Jinwoo Shin</a>,
                    <a href="https://web.eecs.umich.edu/~honglak/">Honglak Lee</a>,
                    <a href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
                    <a href="https://sites.google.com/view/kiminlee">Kimin Lee</a>
                    <br>
                    <em>International Conference on Learning Representations (<u>ICLR</u>)</em>, 2022.
                    <br>
                    <a href="https://arxiv.org/abs/2203.10050">Paper</a> / <a
                      href="https://github.com/alinlab/SURF">Code</a>
                    <!-- <a href="https://younggyo.me/">Paper</a> / <a
                      href="https://younggyo.me/">Code</a> -->
                    <p>We introduce semi-supervised learning and temporal data augmentation for improving the
                      feedback-efficiency of preferenced-based RL.</p>
                  </td>
                </tr>

                <tr>
                  <td colspan="2" style="padding:0px">
                    <hr class="soft">
                  </td>
                </tr>

                <tr onmouseout="oreo_stop()" onmouseover="oreo_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/oreo.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function oreo_start() {
                        document.getElementById('oreo_image').style.opacity = "1";
                      }

                      function oreo_stop() {
                        document.getElementById('oreo_image').style.opacity = "0";
                      }
                      oreo_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2110.14118">
                      <papertitle>Object-Aware Regularization for Addressing Causal Confusion in Imitation Learning
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://scholar.google.co.kr/citations?user=F9DGEgEAAAAJ&hl=en">Jongjin Park*</a>,
                    <strong>Younggyo Seo*</strong>,
                    <a href="https://changliu00.github.io/">Chang Liu</a>,
                    <a href="https://www.microsoft.com/en-us/research/people/lizo/">Li Zhao</a>,
                    <a href="https://stepjam.github.io/">Tao Qin</a>,
                    <a href="https://alinlab.kaist.ac.kr/shin.html">Jinwoo Shin</a>,
                    <a href="https://www.microsoft.com/en-us/research/people/tyliu/">Tie-Yan Liu</a>
                    <br>
                    <em>Neural Information Processing Systems (<u>NeurIPS</u>)</em>, 2021.
                    <br>
                    <a href="https://arxiv.org/abs/2110.14118">Paper</a> / <a
                      href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/articles/object-aware-regularization-for-addressing-causal-confusion-in-imitation-learning/">Article</a>
                    / <a href="https://github.com/alinlab/oreo">Code</a>
                    <p>We introduce OREO, a regularization technique for behavior cloning from pixels.</p>
                  </td>
                </tr>

                <tr onmouseout="higl_stop()" onmouseover="higl_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/higl.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function higl_start() {
                        document.getElementById('higl_image').style.opacity = "1";
                      }

                      function higl_stop() {
                        document.getElementById('higl_image').style.opacity = "0";
                      }
                      higl_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2110.13625">
                      <papertitle>Landmark-Guided Subgoal Generation in Hierarchical Reinforcement Learning
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://sites.google.com/view/junsu-kim">Junsu Kim</a>,
                    <strong>Younggyo Seo</strong>,
                    <a href="https://alinlab.kaist.ac.kr/shin.html">Jinwoo Shin</a>
                    <br>
                    <em>Neural Information Processing Systems (<u>NeurIPS</u>)</em>, 2021.
                    <br>
                    <a href="https://arxiv.org/abs/2110.13625">Paper</a>
                    / <a href="https://github.com/junsu-kim97/HIGL">Code</a>
                    <p>We introduce HIGL, a goal-conditioned hierarchical RL method that samples landmarks and utilizes
                      them for guiding the training of a high-level policy.</p>
                  </td>
                </tr>

                <tr onmouseout="off2on_stop()" onmouseover="off2on_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/off2on.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function off2on_start() {
                        document.getElementById('off2on_image').style.opacity = "1";
                      }

                      function off2on_stop() {
                        document.getElementById('off2on_image').style.opacity = "0";
                      }
                      off2on_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2107.00591">
                      <papertitle>Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic
                        Q-Ensemble
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://sites.google.com/view/seunghyun-lee/home">Seunghyun Lee*</a>,
                    <strong>Younggyo Seo*</strong>,
                    <a href="https://sites.google.com/view/kiminlee">Kimin Lee</a>,
                    <a href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
                    <a href="https://alinlab.kaist.ac.kr/shin.html">Jinwoo Shin</a>
                    <br>
                    <em>Conference on Robot Learning (<u>CoRL</u>)</em>, 2021.
                    <br>
                    <em>NeurIPS Workshop on Offline RL</em>, 2020 as <b>Oral presentation</b>.
                    <br>
                    <a href="https://arxiv.org/abs/2107.00591">Paper</a>
                    / <a href="https://github.com/shlee94/Off2OnRL">Code</a>
                    <p>We introduce an offline-to-online RL algorithm to address the distribution shift that arises
                      during
                      the transition between offline RL and online RL.</p>
                  </td>
                </tr>

                <tr onmouseout="re3_stop()" onmouseover="re3_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/re3.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function re3_start() {
                        document.getElementById('re3_image').style.opacity = "1";
                      }

                      function re3_stop() {
                        document.getElementById('re3_image').style.opacity = "0";
                      }
                      re3_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2102.09430">
                      <papertitle>State Entropy Maximization with Random Encoders for Efficient Exploration
                      </papertitle>
                    </a>
                    <br>
                    <strong>Younggyo Seo*</strong>,
                    <a href="http://www.lilichen.me/">Lili Chen*</a>,
                    <a href="http://alinlab.kaist.ac.kr/shin.html">Jinwoo Shin</a>,
                    <a href="https://web.eecs.umich.edu/~honglak/">Honglak Lee</a>,
                    <a href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
                    <a href="https://sites.google.com/view/kiminlee">Kimin Lee</a>
                    <br>
                    <em>International Conference on Machine Learning (<u>ICML</u>)</em>, 2021.
                    <br>
                    <a href="https://arxiv.org/abs/2102.09430">Paper</a> / <a
                      href="https://sites.google.com/view/re3-rl">Website</a> / <a
                      href="https://github.com/younggyoseo/re3">Code</a>
                    <p>We introduce RE3, an exploration technique for visual RL that utilizes a k-NN state entropy
                      estimate in the representation space of a randomly initialized & fixed CNN encoder. </p>
                  </td>
                </tr>

                <tr>
                  <td colspan="2" style="padding:0px">
                    <hr class="soft">
                  </td>
                </tr>

                <tr onmouseout="tmcl_stop()" onmouseover="tmcl_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/tmcl.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function tmcl_start() {
                        document.getElementById('tmcl_image').style.opacity = "1";
                      }

                      function tmcl_stop() {
                        document.getElementById('tmcl_image').style.opacity = "0";
                      }
                      tmcl_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2010.13303">
                      <papertitle>Trajectory-wise Multiple Choice Learning for Dynamics Generalization in Reinforcement
                        Learning
                      </papertitle>
                    </a>
                    <br>
                    <strong>Younggyo Seo*</strong>,
                    <a href="https://sites.google.com/view/kiminlee">Kimin Lee*</a>,
                    <a href="https://iclavera.github.io/">Ignasi Clavera</a>,
                    <a href="https://scholar.google.com/citations?user=IbcqwaoAAAAJ&hl=en">Thanard Kurutach</a>,
                    <a href="http://alinlab.kaist.ac.kr/shin.html">Jinwoo Shin</a>,
                    <a href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>
                    <br>
                    <em>Neural Information Processing Systems (<u>NeurIPS</u>)</em>, 2020.
                    <br>
                    <a href="https://arxiv.org/abs/2010.13303">Paper</a> / <a
                      href="https://sites.google.com/view/trajectory-mcl">Website</a> / <a
                      href="https://github.com/younggyoseo/trajectory_mcl">Code</a>
                    <p>We introduce T-MCL that learns multi-headed dynamics model whose each prediction head is
                      specialized in certain environments with similar dynamics, i.e., clustering environments.</p>
                  </td>
                </tr>

                <tr onmouseout="cadm_stop()" onmouseover="cadm_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/cadm.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function cadm_start() {
                        document.getElementById('cadm_image').style.opacity = "1";
                      }

                      function cadm_stop() {
                        document.getElementById('cadm_image').style.opacity = "0";
                      }
                      cadm_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2005.06800">
                      <papertitle>Context-aware Dynamics Model for Generalization in Model-Based Reinforcement Learning
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://sites.google.com/view/kiminlee">Kimin Lee*</a>,
                    <strong>Younggyo Seo*</strong>,
                    <a href="https://sites.google.com/view/seunghyun-lee/home">Seunghyun Lee</a>,
                    <a href="https://web.eecs.umich.edu/~honglak/">Honglak Lee</a>,
                    <a href="http://alinlab.kaist.ac.kr/shin.html">Jinwoo Shin</a>
                    <br>
                    <em>International Conference on Machine Learning (<u>ICML</u>)</em>, 2020.
                    <br>
                    <a href="https://arxiv.org/abs/2005.06800">Paper</a> / <a
                      href="https://sites.google.com/view/cadm">Website</a> / <a
                      href="https://github.com/younggyoseo/CaDM">Code</a>
                    <p>We introduce CaDM that learns a context encoder to extract contextual information from
                      recent history with self-supervised losses of future and backward prediction.</p>
                  </td>
                </tr>

                <tr onmouseout="lwd_stop()" onmouseover="lwd_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/lwd.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function lwd_start() {
                        document.getElementById('lwd_image').style.opacity = "1";
                      }

                      function lwd_stop() {
                        document.getElementById('lwd_image').style.opacity = "0";
                      }
                      lwd_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2006.09607">
                      <papertitle>Learning What to Defer for Maximum Independent Sets
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://sungsooahn.notion.site/">Sungsoo Ahn</a>,
                    <strong>Younggyo Seo</strong>,
                    <a href="http://alinlab.kaist.ac.kr/shin.html">Jinwoo Shin</a>
                    <br>
                    <em>International Conference on Machine Learning (<u>ICML</u>)</em>, 2020.
                    <br>
                    <a href="https://arxiv.org/abs/2006.09607">Paper</a> / <a
                      href="https://github.com/sungsoo-ahn/learning_what_to_defer">Code</a>
                    <p>We introduce LwD, a deep RL framework for the maximum independent set problem.</p>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>
          <hr class="soft">

          <button style="border:0px transparent; background-color: transparent;outline:none;" type="button"
            class="collapsible" data-toggle="collapse" data-target="#content-experience" id="experience">
            <heading>Experience</heading>
          </button>
          <div id="content-experience" class="collapse in">

            <table border=0 class="bg_colour"
              style="padding:20px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>

                <tr>
                  <td style="padding:10px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/amazon_logo.png' width="120">
                    </div>
                  </td>
                  <td style="padding:10px;width:75%;vertical-align:top">
                    <papertitle style="color:gray"><big>Applied Scientist</big> </papertitle>
                    <papertitle><big> | Amazon</big></papertitle>
                    <br>
                    2025 July - Current (San Francisco, CA)
                    <br>
                    <br>
                    <p>Frontier AI and Robotics team (working with <a
                        href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter
                        Abbeel</a>)</p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:10px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/berkeley_logo.png' width="120">
                    </div>
                  </td>
                  <td style="padding:10px;width:75%;vertical-align:top">
                    <papertitle style="color:gray"><big>Postdoctoral Scholar</big> </papertitle>
                    <papertitle><big> | UC Berkeley</big></papertitle>
                    <br>
                    2024 Oct - 2025 June (Berkeley, CA)
                    <br>
                    <br>
                    <p>Berkeley Robot Learning Lab (working with <a
                        href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter
                        Abbeel</a>)</p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:10px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/dyson_logo.jpg' width="120">
                    </div>
                  </td>
                  <td style="padding:10px;width:75%;vertical-align:top">
                    <papertitle style="color:gray"><big>Research Scientist </big> </papertitle>
                    <papertitle><big> | Dyson Robot Learning Lab</big></papertitle>
                    <br>
                    2023 July - 2024 Oct (London, UK)
                    <br>
                    <br>
                    <p>Dyson Robot Learning Lab (working with <a href="https://stepjam.github.io/">Stephen James</a>)
                    </p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:10px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/berkeley_logo.png' width="120">
                    </div>
                  </td>
                  <td style="padding:10px;width:75%;vertical-align:top">
                    <papertitle style="color:gray"><big>Visiting Scholar </big> </papertitle>
                    <papertitle><big> | UC Berkeley</big></papertitle>
                    <br>
                    2021 Oct - 2022 July (Berkeley, CA)
                    <br>
                    <br>
                    <p>Berkeley Robot Learning Lab (working with <a
                        href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter
                        Abbeel</a> and <a href="https://sites.google.com/view/kiminlee">Kimin Lee</a>)</p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:10px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/microsoft_logo.jpg' width="120">
                    </div>
                  </td>
                  <td style="padding:10px;width:75%;vertical-align:top">
                    <papertitle style="color:gray"><big>Research Intern</big> </papertitle>
                    <papertitle><big> | Microsoft Research Asia</big></papertitle>
                    <br>
                    Dec 2020 - May 2021 (Remote)
                    <br>
                    <br>
                    <p>Deep and Reinforcement
                      Learning Group (working with <a href="https://changliu00.github.io/">Chang Liu</a>, <a
                        href="https://www.microsoft.com/en-us/research/people/lizo/">Li Zhao</a>, and <a
                        href="https://www.microsoft.com/en-us/research/people/taoqin/">Tao Qin</a>)</p>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>
          <hr class="soft">

          <button style="border:0px transparent; background-color: transparent;outline:none;" type="button"
            class="collapsible" data-toggle="collapse" data-target="#content-education" id="education">
            <heading>Education</heading>
          </button>
          <div id="content-education" class="collapse in">

            <table border=0 class="bg_colour"
              style="padding:20px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>

                <tr>
                  <td style="padding:10px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/kaist_logo.png' width="120">
                    </div>
                  </td>
                  <td style="padding:10px;width:75%;vertical-align:top">
                    <papertitle style="color:gray"><big>Ph.D in Artificial Intelligence</big> </papertitle>
                    <papertitle><big> | KAIST</big></papertitle>
                    <br>
                    Sep 2019 - Aug 2023
                    <br>
                    <br>
                    <ul>
                      <li>M.S / Ph.D Integrated Program</li>
                      <li>Advisor: <a href="https://alinlab.kaist.ac.kr/shin.html">Jinwoo Shin</a></li>
                    </ul>
                  </td>
                </tr>

                <tr>
                  <td style="padding:10px;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/snu_logo.png' width="120">
                    </div>
                  </td>
                  <td style="padding:10px;width:75%;vertical-align:top">
                    <papertitle style="color:gray"><big>B.A in Economics</big> </papertitle>
                    <papertitle><big> | Seoul National University</big></papertitle>
                    <br>
                    Mar 2012 - Feb 2019
                    <br>
                    <br>
                    <ul>
                      <li>Summa Cum Laude</li>
                      <li>Leave of absence for military service: Feb 2014 - Feb 2016</li>
                    </ul>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>
          <hr class="soft">

          <button style="border:0px transparent; background-color: transparent;outline:none;" type="button"
            class="collapsible" data-toggle="collapse" data-target="#content-academic" id="academic">
            <heading>Academic Activities</heading>
          </button>
          <div id="content-academic" class="collapse in">
            <table border=0 class="bg_colour"
              style="padding:20px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>

                <tr>
                  <td>
                    <ul>
                      <li>Area Chair & Reviewer
                        <ul>
                          <li>Area Chair: NeurIPS</li>
                          <li>Conference Reviewer: ICLR, ICML, NeurIPS, CoRL, CVPR, AAAI, IROS, RSS</li>
                          <li>Journal Reviewer: TMLR, IEEE RA-L</li>
                        </ul>
                      </li>
                      <li>Workshop Organizer
                        <ul>
                          <li><a href="https://sites.google.com/view/corl2022-prl/home">1st Pre-training
                              Robot Learning Workshop</a> @ CoRL2022</li>
                          <li><a href="https://sites.google.com/view/corl2023-prl/home">2nd Pre-training
                              Robot Learning Workshop</a> @ CoRL2023</li>
                        </ul>
                      </li>
                    </ul>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>
          <hr class="soft">

          <button style="border:0px transparent; background-color: transparent;outline:none;" type="button"
            class="collapsible" data-toggle="collapse" data-target="#content-award" id="award">
            <heading>Honors & Awards</heading>
          </button>
          <div id="content-award" class="collapse in">
            <table border=0 class="bg_colour"
              style="padding:20px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>

                <tr>
                  <td>
                    <ul>
                      <li>Top Reviewer Award (top 10%), ICML 2021, 2022</li>
                      <li>AI/CS/EE Rising Stars Award, Google Explore Computer Science Research, 2022</li>
                      <li>Best Paper Award, Korean Artificial Intelligence Association, 2021</li>
                      <li>Summa Cum Laude, Economics department, Seoul National University, 2019</li>
                      <li>National Humanities Scholarship, Korea Student Aid Foundation, 2012-2018</li>
                      <li>1st Rank @ College Scholastic Ability Test with perfect score (500/500), Korea, 2011</li>
                    </ul>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>
          <hr class="soft">

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Website templates from <a href="https://jonbarron.info/" target="_blank">here</a> and <a
                      href="https://rishabkhincha.github.io/" target="_blank">here</a>.</p>
                </td>
                </p>
        </td>
      </tr>
    </tbody>
  </table>
  </td>
  </tr>
  </table>
</body>

</html>